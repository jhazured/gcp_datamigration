# -------------------------------
# ðŸŸ¢ Builder stage
# -------------------------------
FROM python:3.10-slim-bullseye AS builder

# Install Java and minimal build tools
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
      openjdk-11-jre-headless \
      curl ca-certificates && \
    rm -rf /var/lib/apt/lists/*

ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
ENV PATH=$JAVA_HOME/bin:$PATH

# Install Spark manually using curl
ENV SPARK_VERSION=3.5.1
ENV HADOOP_VERSION=3
ENV SPARK_HOME=/opt/spark
ENV PATH=$SPARK_HOME/bin:$PATH

RUN curl -sSL https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \
    | tar -xz -C /opt && \
    mv /opt/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} ${SPARK_HOME}

WORKDIR /install

COPY requirements/prod.txt .

RUN pip install --upgrade pip && \
    pip install --no-cache-dir -r prod.txt

# -------------------------------
# ðŸŸ¢ Final runtime stage
# -------------------------------
FROM python:3.10-slim-bullseye

# Copy Java & Spark from builder
COPY --from=builder /usr/lib/jvm /usr/lib/jvm
COPY --from=builder /opt/spark /opt/spark
COPY --from=builder /usr/local/lib/python3.10/site-packages /usr/local/lib/python3.10/site-packages

ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
ENV SPARK_HOME=/opt/spark
ENV PATH=$JAVA_HOME/bin:$SPARK_HOME/bin:$PATH

RUN useradd -ms /bin/bash etl_user

WORKDIR /app

# Copy the app source
COPY . .

RUN chmod +x scripts/tasks.sh scripts/run_pytest.sh scripts/run_bash.sh && \
    chown -R etl_user:etl_user /app

USER etl_user

EXPOSE 4040

CMD ["python", "framework/main.py"]
